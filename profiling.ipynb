{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3f3a8-5bdd-4cc7-98d3-a64c69a96bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63e23e-6375-4885-9535-aa397fc4821a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_id = '11mlabs/indri-0.1-124m-tts'\n",
    "model_id = 'openai-community/gpt2'\n",
    "# model_id = 'Qwen/Qwen2.5-Coder-7B-Instruct'\n",
    "device = 'cuda:0'\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\"\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f5788-a3eb-4ce7-931c-b8dedeb44186",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = torch.compile(llm)#, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a55aa7-0cf9-45d3-b8a3-c7dff9383226",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is indri\",\n",
    "    \"The president of the United States is donald trump\",\n",
    "    \"The capital of France is paris\",\n",
    "    \"The future of AI is too much overhyped\",\n",
    "]\n",
    "# p|rompts = ['write a program to solve fibonacci series']\n",
    "# prompts = [f'[text]{p}[convert][mimi][spkr_52]' for p in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb78687-bcc1-4f3b-a5ec-f2490de79174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer(prompts[0], return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff6fed-d8d9-42a1-a2bc-dad044522192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipykernel_launcherfor _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        o = llm.generate(**tokens, max_new_tokens=256)\n",
    "        print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3aa67-cd14-43c3-bc0c-8ac8be0ea543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_times = []\n",
    "out_tokens = []\n",
    "gen_speed = []\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    with torch.no_grad():\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "        start_event.record()\n",
    "        # start_time = time.time()\n",
    "    \n",
    "        out = llm.generate(**tokens, max_new_tokens=256)\n",
    "    \n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "        inference_time = start_event.elapsed_time(end_event) / 1000.0\n",
    "        # inference_time = (time.time() - start_time)\n",
    "\n",
    "        out_tokens.append(out.shape[-1] - tokens['input_ids'].shape[-1])\n",
    "        inference_times.append(inference_time)\n",
    "        gen_speed.append(out_tokens[-1] / inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49848a6-eede-4c40-a452-4e1eac991ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Calculate averages\n",
    "avg_out_tokens = np.mean(out_tokens)\n",
    "avg_gen_speed = np.mean(gen_speed)\n",
    "\n",
    "# Plot out_tokens on the first y-axis\n",
    "color1 = 'tab:blue'\n",
    "ax1.set_xlabel('Run Number')\n",
    "ax1.set_ylabel('Generated Tokens', color=color1)\n",
    "ax1.plot(out_tokens, color=color1, marker='o', label='Tokens')\n",
    "ax1.axhline(y=avg_out_tokens, color=color1, linestyle='--', label='Avg Tokens')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "# Create a second y-axis for generation speed\n",
    "ax2 = ax1.twinx()\n",
    "color2 = 'tab:red'\n",
    "ax2.set_ylabel('Generation Speed (Tokens/Second)', color=color2)\n",
    "ax2.plot(gen_speed, color=color2, marker='s', label='Speed')\n",
    "ax2.axhline(y=avg_gen_speed, color=color2, linestyle='--', label='Avg Speed')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "plt.title('Generated Tokens and Generation Speed')\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ac34b-4c55-414c-87a6-5ff9705cd656",
   "metadata": {},
   "source": [
    "Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b2d9f-4234-4b6e-8a41-be09e7cb1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037285e-0ec2-449e-821b-3578a0d0dd06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    # record_shapes=True,\n",
    "    # profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = llm.generate(**tokens, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb43c3-3a26-4c81-8082-e0a41635cdf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"trace_coder7b_compile.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac175fc3-d9b2-4b3f-8f87-c019c90dda87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('------------------------------------------------------------')\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n",
    "print('------------------------------------------------------------')\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f034fd-5740-4783-924d-231cc14f7636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
