# indri-benchmark

Making small indri as fast as possible

This is an effort at making small models (<1B params) run as fast as possible!
(Running models at the highest throughput possible and lowest time to fist token for the batch size of 1)

- Baseline numbers on 4090 and A100
- Why is it so slow?
  - What is the expected?
  - Why is it lower than expected?
  - How do verify your hunch?
- Special case with smaller models vs larger models
  - Why?
- How to go about speeding it up?
  - Remove dead code
  - CUDA graphs
  - Sampling with custom kernels
  - Logit processor with custom kernels
- Final results


## Baseline numbers

### 4090

Let's first evaluate what speed do we get for small models on consumer grade GPUs. For this test, I am selecting Nvidia RTX 4090 as the GPU and gpt2 (smallest size) as the model. For simplicity, I am selecting batchsize as 1. We can extend this for multiple batch sizes. We will examine two frameworks - vLLM and HuggingFace. The model run via HuggingFace is first compiled using `torch.compile`.

1. TTFT (time to first token): We will keep the output tokens fixed and vary the number of input tokens given to the model.
2. Throughput (Tokens/s): We will keep the input tokens fixed and vary output tokens generated by the model.

Figure 1: (Left) TTFT vs # of input tokens, (Right) Decoding throughput vs # of output tokens

A couple of observations from the benchmakr run:

1. The TTFT for huggingface is unusually lower than vLLM for small number of input tokens, but a general observation is that vLLM's TTFT is much more stable and increases less HuggingFace.
2. The decoding throughput of vLLM is ~2x of HuggingFace. This clearly shows the effor that vLLM team has put into optimizing inference for language models.

## Why is it slow?

Let's first figure out if the number we get are the best numbers we can get. While autoregressive decoding, we are doing 2 things:

1. Computations e.g. matrix multiplacations. The latency of these computations are bound by FLOPS of the GPU
2. Transferring model weights to and fro from the global memory of the GPU (HBM) to L1 cache where the cores can access them. The latency of this movement is bound by the memory bandwidth of the GPU.

The end to end latency is bound by whichever is slower from the two ($Latency_{compute}$, $Latency_{memory}$)

Let's calculate theoretical latencies on Nvidia 4090 for GPT model. There are two phases in autoregressive decoding:

1. Prefill phase: Where we process the prompt and generate the first token
2. Decoding phase: Where we generate the remaining tokens auto regressively

### Prefill phase

For prefill phase, back of the napkin math suggests that we need $2*P FLOPS$ for generating the first token. For our gpt-2 small model (137M params.), it comes to around:

$$2 * 137 * 10^6 \approx 2.7 GFlOPSs$$

Nvidia 4090 has 84 TFlops of compute for FP16. Assuming 100% utilization, we would get

$$Latency_{compute} = 2.7 GFLOPS/84 TFLOPS = 3.3 * 10^{-5}s$$

> You can use `torch.flop_counter` to get the exact FLOPs of GPT2. The one we estimate here is close enough to the actual number.

Similarly, assuming FP16 weights, the model size that the GPU would need to transfer would be $2*137*10^6 \approx 274 MB$. The memory bandwidth of Nvidia 4090 is $1008 GB/s$. We would get:

$$Latency_{memory} = 274 MB/1008 (GB/s) = 2.7 * 10^{-4}s$$

The prefill phase of the model will be bound by $min(Latency_{compute}, Latency_{memory})$ which for this case comes to be memory bound. Theoretically, we should get the latency of 0.27 ms for prefill phase (i.e. TTFT).

<!-- some math to prove it should run at >3k tok/s -->

So what's the reason for this model running slower than expected?


## References
